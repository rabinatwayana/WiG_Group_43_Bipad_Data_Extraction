{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install urllib3\n",
    "# !pip install idna\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://bipadportal.gov.np/api/v1/province/\n",
    "# https://bipadportal.gov.np/api/v1/district/\n",
    "# https://bipadportal.gov.np/api/v1/municipality/\n",
    "# https://bipadportal.gov.np/api/v1/hazard/\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "def get_data_from_api(api_url):\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx and 5xx status codes\n",
    "        data = response.json()  # Convert response to JSON format\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching data:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HAZARD LIST\n",
    "datas = get_data_from_api('https://bipadportal.gov.np/api/v1/hazard/')\n",
    "natural_hazard_dataset=[]\n",
    "non_natural_hazard_dataset=[]\n",
    "if datas:\n",
    "    for instance in datas['results']:\n",
    "        if instance['type']==\"natural\":\n",
    "            natural_hazard_dataset.append({\n",
    "                \"id\":instance['id'],\n",
    "                \"title\":instance['title'],\n",
    "                \"type\":instance['type'],\n",
    "            })\n",
    "        else:\n",
    "            non_natural_hazard_dataset.append({\n",
    "                \"id\":instance['id'],\n",
    "                \"title\":instance['title'],\n",
    "                \"type\":instance['type'],\n",
    "            })\n",
    "        \n",
    "else:\n",
    "    print(\"empty data\")\n",
    "    \n",
    "natural_hazard_dataset=sorted(natural_hazard_dataset, key=lambda x: x['id'])\n",
    "non_natural_hazard_dataset=sorted(non_natural_hazard_dataset, key=lambda x: x['id'])\n",
    "    \n",
    "# print(natural_hazard_dataset)\n",
    "# print(non_natural_hazard_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {'id': 2, 'title': 'Animal Incidents', 'type': 'natural'}, \n",
    "    {'id': 3, 'title': 'Avalanche', 'type': 'natural'}, \n",
    "    {'id': 6, 'title': 'Cold Wave', 'type': 'natural'}, \n",
    "    {'id': 8, 'title': 'Earthquake', 'type': 'natural'}, \n",
    "    {'id': 9, 'title': 'Epidemic', 'type': 'natural'}, \n",
    "    {'id': 10, 'title': 'Fire', 'type': 'natural'}, \n",
    "    {'id': 11, 'title': 'Flood', 'type': 'natural'}, \n",
    "    {'id': 12, 'title': 'Forest Fire', 'type': 'natural'}, \n",
    "    {'id': 13, 'title': 'Hailstorm', 'type': 'natural'}, \n",
    "    {'id': 14, 'title': 'Heavy Rainfall', 'type': 'natural'}, \n",
    "    {'id': 17, 'title': 'Landslide', 'type': 'natural'}, \n",
    "    {'id': 18, 'title': 'Other (Natural)', 'type': 'natural'}, \n",
    "    {'id': 19, 'title': 'Rainfall', 'type': 'natural'}, \n",
    "    {'id': 21, 'title': 'Snow Storm', 'type': 'natural'}, \n",
    "    {'id': 22, 'title': 'Storm', 'type': 'natural'}, \n",
    "    {'id': 23, 'title': 'Thunderbolt', 'type': 'natural'}, \n",
    "    {'id': 24, 'title': 'Wind Storm', 'type': 'natural'}, \n",
    "    {'id': 25, 'title': 'Drought', 'type': 'natural'}, \n",
    "    {'id': 26, 'title': 'Glacial lake outburst', 'type': 'natural'}, \n",
    "    {'id': 27, 'title': 'Heat wave', 'type': 'natural'}, \n",
    "    {'id': 28, 'title': 'Inundation', 'type': 'natural'}, \n",
    "    {'id': 29, 'title': 'Soil Erosion', 'type': 'natural'}, \n",
    "    {'id': 30, 'title': 'Volcanic eruption', 'type': 'natural'}, \n",
    "    {'id': 38, 'title': 'Famine', 'type': 'natural'}\n",
    "]\n",
    "\n",
    "#hazard_id and title dict\n",
    "\n",
    "hazard_dict={\n",
    " 2: 'Animal Incidents', \n",
    " 3: 'Avalanche', \n",
    " 6: 'Cold Wave', \n",
    " 8: 'Earthquake', \n",
    " 9: 'Epidemic', \n",
    "    10: 'Fire', \n",
    "    11: 'Flood', \n",
    "    12: 'Forest Fire', \n",
    "    13: 'Hailstorm', \n",
    "    14: 'Heavy Rainfall', \n",
    "    17: 'Landslide', \n",
    "    18: 'Other (Natural)', \n",
    "    19: 'Rainfall', \n",
    "    21: 'Snow Storm', \n",
    "    22: 'Storm', \n",
    "    23: 'Thunderbolt', \n",
    "    24: 'Wind Storm', \n",
    "    25: 'Drought', \n",
    "    26: 'Glacial lake outburst', \n",
    "    27: 'Heat wave', \n",
    "    28: 'Inundation', \n",
    "    29: 'Soil Erosion', \n",
    "    30: 'Volcanic eruption', \n",
    "    38: 'Famine', 'type': 'natural'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://bipadportal.gov.np/api/v1/incident/?rainBasin=&rainStation=&riverBasin=&riverStation=&hazard=12&incident_on__gt=2000-01-01T00%3A00%3A00%2B05%3A45&incident_on__lt=2024-05-31T23%3A59%3A59%2B05%3A45&expand=loss%2Cevent%2Cwards&ordering=-incident_on&limit=-1&data_source=drr_api\n",
    "\n",
    "# https://bipadportal.gov.np/api/v1/incident/?hazard=12&incident_on__gt=2000-01-01T00%3A00%3A00%2B05%3A45&incident_on__lt=2024-05-31T23%3A59%3A59%2B05%3A45&ordering=incident_on&limit=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL CODE START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exclude_hazard_ids=[2,3,6,8,9,10,11,12,13,14,17,18,19,21,22,23,24,25,26,27,28,29,30,38] #original set\n",
    "exclude_hazard_ids=[13,18,19,25,26,27,28,29,30,38]\n",
    "\n",
    "# no_data=[13,18,19,25,26,27,28,29,30,38]\n",
    "\n",
    "total_hazard_count=0\n",
    "\n",
    "for hazard in natural_hazard_dataset:\n",
    "    year_wise_incicent_counts_df=pd.DataFrame()\n",
    "    hazard_id=hazard['id']\n",
    "    try:\n",
    "        if hazard_id in exclude_hazard_ids:\n",
    "            continue\n",
    "        print(f\"*********{hazard_id}********\")\n",
    "        sheet_data={}\n",
    "        year_wise_incident_count_sheet={}\n",
    "        province_ids=[1,2,3,4,5,6,7]\n",
    "        continue_check=False\n",
    "        drop_column_list=['wards','titleNe','event','hazard','loss','createdBy','updatedBy','region','regionId','dataSourceId','dataSource','createdOn', 'modifiedOn','point']\n",
    "        for province_id in province_ids:\n",
    "            \n",
    "            api_url=f\"https://bipadportal.gov.np/api/v1/incident/?hazard={hazard_id}&province={province_id}&incident_on__gt=2000-01-01T00%3A00%3A00%2B05%3A45&incident_on__lt=2024-05-31T23%3A59%3A59%2B05%3A45&ordering=incident_on\"\n",
    "            datas = get_data_from_api(api_url)\n",
    "            if datas:\n",
    "                data_list=datas['results']\n",
    "                if len(data_list)==0:\n",
    "                    # print(province_id)\n",
    "                    continue\n",
    "                # print(data_list)\n",
    "                total_hazard_count+=len(data_list)\n",
    "                # print(len(data_list))\n",
    "                df = pd.DataFrame(data_list)\n",
    "                try:\n",
    "                    df['latitude'] = df['point'].apply(lambda x: x['coordinates'][1])\n",
    "                    df['longitude'] = df['point'].apply(lambda x: x['coordinates'][0])\n",
    "                except Exception as e:\n",
    "                    print(\"error lat lng\",str(e))\n",
    "                    continue_check=True\n",
    "                    # continue\n",
    "                try:\n",
    "                    reported_on_dt = df['reportedOn'].str.split('T')\n",
    "                    df['reportedOn'] = reported_on_dt.str[0]\n",
    "                    df['reportedOnTime'] = reported_on_dt.str[1]\n",
    "                except Exception as e:\n",
    "                    print(\"error reported on\",str(e))\n",
    "                    continue_check=True\n",
    "                    # continue\n",
    "                try:\n",
    "                    incident_on_dt = df['incidentOn'].str.split('T')\n",
    "                    df['incidentOn'] = incident_on_dt.str[0]\n",
    "                    df['incidentOnTime'] = incident_on_dt.str[1]\n",
    "                    \n",
    "                    df['incidentOn'] = pd.to_datetime(df['incidentOn'], format='%Y-%m-%d')\n",
    "                    # Extract the year and create a new column 'year'\n",
    "                    df['year'] = df['incidentOn'].dt.year\n",
    "                except Exception as e:\n",
    "                    print(\"error incident_on\",str(e))\n",
    "                    continue_check=True\n",
    "                    # continue\n",
    "\n",
    "                drop_column_list = [col for col in drop_column_list if col in df.columns]\n",
    "                \n",
    "                df.drop(drop_column_list, axis=1, inplace=True)\n",
    "\n",
    "                try:\n",
    "                    year_wise_incicent_counts_df = df.groupby('year').size()\n",
    "                    \n",
    "                    # Calculate the total occurrence of each unique year\n",
    "                    year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "                    # Convert the result to a DataFrame\n",
    "                    year_wise_incicent_counts_df = year_counts.reset_index()\n",
    "                    year_wise_incicent_counts_df.columns = ['year', hazard['title']]\n",
    "                    year_wise_incident_count_sheet[f\"province_{province_id}\"]=year_wise_incicent_counts_df\n",
    "                except Exception as e:\n",
    "                    print(\"error count\",str(e))\n",
    "                    continue_check=True\n",
    "                    # continue\n",
    "                \n",
    "                sheet_data[f\"province_{province_id}\"]=df\n",
    "        if continue_check==True:\n",
    "            print(\"continue check true\")\n",
    "            # continue\n",
    "        output_path=f\"/Users/rabina/Projects/Bipad_data_extraction/output_1/{hazard['id']}_{hazard['title']}_provincewise.xlsx\"\n",
    "        output_path_2=f\"/Users/rabina/Projects/Bipad_data_extraction/output_2/{hazard['id']}_{hazard['title']}_year_wise_incident_count.xlsx\"\n",
    "        \n",
    "        # Create an Excel writer object\n",
    "        if not df.empty:\n",
    "            with pd.ExcelWriter(output_path) as writer:\n",
    "                # Iterate over the dictionary and write each DataFrame to the corresponding sheet\n",
    "                for sheet_name, data_frame in sheet_data.items():\n",
    "                    data_frame.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        if not year_wise_incicent_counts_df.empty:\n",
    "            with pd.ExcelWriter(output_path_2) as writer:\n",
    "                # Iterate over the dictionary and write each DataFrame to the corresponding sheet\n",
    "                for sheet_name, data_frame in year_wise_incident_count_sheet.items():\n",
    "                    data_frame.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"error in hazard id {hazard_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"TOTAL HAZARD COUNT\",total_hazard_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGING ALL HAZARDS\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = '/Users/rabina/Projects/Bipad_data_extraction/output_2'\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Initialize a dictionary to hold combined DataFrames for each province\n",
    "combined_province_dfs = {f'province_{i}': pd.DataFrame() for i in range(1, 8)}\n",
    "# print(combined_province_dfs)\n",
    "sheet_names = list(combined_province_dfs.keys())\n",
    "\n",
    "for file in files:\n",
    "    hazard_id = int(file.split(\"_\")[0])\n",
    "    hazard_name = hazard_dict[hazard_id]\n",
    "\n",
    "    file_path = os.path.join(directory, file)\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        try:\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        except:\n",
    "            continue\n",
    "        combined_df = combined_province_dfs[sheet_name]\n",
    "\n",
    "        if combined_df.empty:\n",
    "            combined_province_dfs[sheet_name] = df\n",
    "        else:\n",
    "            if hazard_name not in combined_df.columns:\n",
    "                combined_province_dfs[sheet_name] = pd.merge(combined_df, df, on='year', how='outer')\n",
    "\n",
    "# Save the combined DataFrames to an Excel file\n",
    "output_path_3 = \"/Users/rabina/Projects/Bipad_data_extraction/provine_wise_hazard_combined_sheet.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path_3) as writer:\n",
    "    for sheet_name, data_frame in combined_province_dfs.items():\n",
    "        data_frame.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## FINAL CODE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
